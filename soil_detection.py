# -*- coding: utf-8 -*-
"""soil detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eDlh8OLNGhElbMiEllFkfrgqW5TqE0E7
"""

import subprocess
subprocess.run(["pip", "install", "seaborn"], check=True)
from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import numpy as np
import requests
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta, timezone
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import random # Moved import random to the top

API_KEY = "c21d57243acb8f03fdf8a17b41a19edb"  # Replace with your real key
CITY = "Chennai"
ORIGINAL_CSV_PATH = "seoul 2022-01-01 to 2024-01-01.csv"
FORECAST_CSV = "7_day_forecast_seoul.csv"

# Attempt to load the original CSV, if not found, create dummy data
try:
    df = pd.read_csv(ORIGINAL_CSV_PATH)
    CSV_PATH = ORIGINAL_CSV_PATH # Keep CSV_PATH as original if found
    print(f"✅ Successfully loaded {ORIGINAL_CSV_PATH}")
except FileNotFoundError:
    print(f"❌ {ORIGINAL_CSV_PATH} not found. Generating dummy weather data.")
    # Generate 20 days of sample weather data
    start_date = datetime.today() - timedelta(days=19)
    dummy_data = []
    conditions = ['Clear', 'Rain', 'Clouds', 'Haze', 'Mist']
    for i in range(20):
        date = (start_date + timedelta(days=i)).date()
        tempmax = round(random.uniform(25, 35), 1)
        tempmin = round(tempmax - random.uniform(3, 6), 1)
        humidity = random.randint(50, 90)
        wind = round(random.uniform(1.0, 5.0), 1)
        condition = random.choice(conditions)
        dummy_data.append([date, tempmax, tempmin, humidity, wind, condition])
    dummy_df = pd.DataFrame(dummy_data, columns=["date", "tempmax", "tempmin", "humidity", "wind", "conditions"])
    DUMMY_WEATHER_CSV_PATH = "dummy_seoul_weather.csv" # A new name for dummy data
    dummy_df.to_csv(DUMMY_WEATHER_CSV_PATH, index=False)
    df = dummy_df # Assign the generated dummy data to df
    CSV_PATH = DUMMY_WEATHER_CSV_PATH # Update CSV_PATH to the dummy file
    print(f"✅ Dummy weather data saved to {DUMMY_WEATHER_CSV_PATH} and loaded.")

df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

if "date" not in df.columns:
    raise ValueError("The dataset must contain a 'date' column.")
# Parse mixed date formats
df["date"] = pd.to_datetime(df["date"], dayfirst=True, errors="coerce")
df = df.dropna(subset=["date"])  # Remove rows where date parsing failed
def fetch_today_weather(city, api_key):
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        return {
            "date": datetime.now(timezone.utc).date().isoformat(), # Updated to use timezone-aware objects
            "tempmax": data["main"]["temp_max"],
            "tempmin": data["main"]["temp_min"],
            "humidity": data["main"]["humidity"],
            "wind": data["wind"]["speed"],
            "conditions": data["weather"][0]["main"]
        }
    else:
        print("❌ Error fetching data:", response.json())
        return None
today_data = fetch_today_weather(CITY, API_KEY)
if today_data and str(today_data["date"]) not in df["date"].astype(str).values:
    df = pd.concat([df, pd.DataFrame([today_data])], ignore_index=True)
    updated_csv_path = "updated_" + CSV_PATH
    df.to_csv(updated_csv_path, index=False)  # ✅ Save to a different file
    CSV_PATH = updated_csv_path # Update CSV_PATH to the new file

    print("✅ Today's weather appended and saved to new file.")
else:
    print("✅ Today's weather already in data or fetch failed. Using existing data.")
df = df.dropna(subset=["tempmax", "tempmin", "humidity", "wind", "conditions"])
le = LabelEncoder()
df["weather_enc"] = le.fit_transform(df["conditions"])
features = ["tempmax", "tempmin", "humidity", "wind", "weather_enc"]
scaler = MinMaxScaler()
scaled = scaler.fit_transform(df[features])
X, y = [], []
time_steps = 7
X, y = [], []
for i in range(len(scaled) - time_steps):
    X.append(scaled[i:i + time_steps])
    y.append(scaled[i + time_steps][0])
X, y = np.array(X), np.array(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
model = Sequential([
    Input(shape=(X.shape[1], X.shape[2])),
    LSTM(64),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')
history = model.fit(X_train, y_train, validation_split=0.2, epochs=30, batch_size=8,
verbose=1)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
last_seq = scaled[-time_steps:]
forecast_scaled = []
input_seq = last_seq.copy()
for _ in range(7):
    next_pred = model.predict(input_seq.reshape(1, time_steps, -1))[0][0]
    new_row = input_seq[-1].copy()
    new_row[0] = next_pred
    input_seq = np.vstack([input_seq[1:], new_row])
    forecast_scaled.append(next_pred)
forecast_temp = scaler.inverse_transform(
    np.hstack([np.array(forecast_scaled).reshape(-1, 1), np.zeros((7, len(features) - 1))])
)[:, 0]

start_date = df["date"].max() + timedelta(days=1)
last_weather = df["weather_enc"].iloc[-1]
forecast_df = pd.DataFrame({
    "date": [start_date + timedelta(days=i) for i in range(7)],
    "predicted_tempmax": np.round(forecast_temp, 2),
    "predicted_weather": [le.inverse_transform([last_weather])[0]] * 7
})

!pip install scikit-learn pandas joblib
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import joblib

# Crop and Fertilizer Prediction section
# Assuming 'data_core.csv' might also be missing. Need to handle this separately if it becomes an issue.
try:
    df_crop_fert = pd.read_csv('data_core.csv') # Using a different DataFrame name to avoid conflict with weather df
    print("✅ Successfully loaded data_core.csv")
    label_encoders = {}
    categorical_cols = ['Soil Type', 'Crop Type', 'Fertilizer Name']
    for col in categorical_cols:
        le = LabelEncoder()
        df_crop_fert[col] = le.fit_transform(df_crop_fert[col])
        label_encoders[col] = le
    X_crop = df_crop_fert[['Temparature', 'Humidity', 'Moisture', 'Soil Type', 'Nitrogen', 'Potassium',
    'Phosphorous']]
    y_crop = df_crop_fert['Crop Type']
    X_train_crop, X_test_crop, y_train_crop, y_test_crop = train_test_split(X_crop, y_crop,
test_size=0.2, random_state=42)
    crop_model = RandomForestClassifier(n_estimators=100, random_state=42)
    crop_model.fit(X_train_crop, y_train_crop)

    y_pred_crop = crop_model.predict(X_test_crop)
    print("Crop Prediction Model Performance:")
    print(classification_report(y_test_crop, y_pred_crop))

    joblib.dump(crop_model, 'crop_model.pkl')
    joblib.dump(label_encoders['Crop Type'], 'crop_label_encoder.pkl')
    X_fert = df_crop_fert[['Temparature', 'Humidity', 'Moisture', 'Soil Type', 'Crop Type', 'Nitrogen',
    'Potassium', 'Phosphorous']]
    y_fert = df_crop_fert['Fertilizer Name']
    X_train_fert, X_test_fert, y_train_fert, y_test_fert = train_test_split(X_fert, y_fert,
test_size=0.2, random_state=42)
    fert_model = RandomForestClassifier(n_estimators=100, random_state=42)
    fert_model.fit(X_train_fert, y_train_fert)
    y_pred_fert = fert_model.predict(X_test_fert)
    print("Fertilizer Prediction Model Performance:")
    print(classification_report(y_test_fert, y_pred_fert))
    joblib.dump(fert_model, 'fertilizer_model.pkl')
    joblib.dump(label_encoders['Fertilizer Name'], 'fertilizer_label_encoder.pkl')
    joblib.dump(label_encoders['Soil Type'], 'soil_label_encoder.pkl')
    print("✅ Models saved successfully: crop_model.pkl & fertilizer_model.pkl")
except FileNotFoundError:
    print("❌ data_core.csv not found. Skipping crop and fertilizer prediction models.")
    # Optionally, you might want to create empty models or a placeholder here if downstream tasks depend on them